{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Detect freshness of fruits using PyTorch\n","<br>\n","<img src='https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png'/>\n","<br>\n","Through this tutorial I will be giving a step by step guide to tackle a simple deep learning problem. In this notebook I will train a model to detect the freshness of the fruit.\n","<br><br>\n","\n","### Now lets first train our model!!"]},{"cell_type":"markdown","metadata":{},"source":["# Importing the Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-07-23T04:14:37.490892Z","iopub.status.busy":"2022-07-23T04:14:37.490245Z","iopub.status.idle":"2022-07-23T04:14:39.512469Z","shell.execute_reply":"2022-07-23T04:14:39.511565Z","shell.execute_reply.started":"2022-07-23T04:14:37.490849Z"},"trusted":true},"outputs":[],"source":["#For accessing files\n","import os\n","import glob\n","\n","#For Images\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","#For checking progress\n","from tqdm import tqdm_notebook\n","\n","import datetime\n","\n","#PyTorch Packages\n","import torch\n","from torch.utils.data.dataset import Dataset\n","from torchvision import transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn as nn\n","\n","#ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["# Studying the data\n","Let's take a look at the data on which we will be training our data and the one on which we will be doing our predictions."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-07-23T04:14:39.515970Z","iopub.status.busy":"2022-07-23T04:14:39.515346Z","iopub.status.idle":"2022-07-23T04:14:39.531254Z","shell.execute_reply":"2022-07-23T04:14:39.530368Z","shell.execute_reply.started":"2022-07-23T04:14:39.515912Z"},"trusted":true},"outputs":[],"source":["def get_image(path,transform=False):\n","    img = cv2.imread(path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    if transform:\n","        img = transform(img)\n","    return img\n","\n","def show_data(rows,cols,is_train=True,transform=False):\n","    if is_train:\n","        path = '/kaggle/input/fruits-fresh-and-rotten-for-classification/dataset/train/'\n","    else:\n","        path = '/kaggle/input/fruits-fresh-and-rotten-for-classification/dataset/test/'\n","    path = os.path.join(path,'*','*.png')\n","    img_paths = glob.glob(path)\n","    np.random.seed(0)\n","    img_paths = np.random.choice(img_paths,rows*cols)\n","    fig = plt.figure(figsize=(8,8),dpi=150)\n","    i = 1\n","    for r in range(rows):\n","        for c in range(cols):\n","            image_path = img_paths[i-1]\n","            if 'fresh' in image_path.split('/')[-2]:\n","                title = 'Fresh'\n","            else:\n","                title = 'Rotten'\n","            ax = fig.add_subplot(rows,cols,i)\n","            img = get_image(image_path,transform)\n","            ax.set_xticks([])\n","            ax.set_yticks([])\n","            ax.set_title(title,fontsize=5)\n","            ax.imshow(img)\n","            i+=1\n","    return fig"]},{"cell_type":"markdown","metadata":{},"source":["## Training data"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-07-23T04:14:39.533168Z","iopub.status.busy":"2022-07-23T04:14:39.532732Z","iopub.status.idle":"2022-07-23T04:14:43.765132Z","shell.execute_reply":"2022-07-23T04:14:43.763069Z","shell.execute_reply.started":"2022-07-23T04:14:39.533124Z"},"trusted":true},"outputs":[],"source":["fig = show_data(5,4)\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["## Test Data\n","We already have the labels so that we can test our model's accuracy. Just showing the labels to show the images in test data."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-07-23T04:14:43.766961Z","iopub.status.busy":"2022-07-23T04:14:43.766600Z","iopub.status.idle":"2022-07-23T04:14:46.856709Z","shell.execute_reply":"2022-07-23T04:14:46.855896Z","shell.execute_reply.started":"2022-07-23T04:14:43.766928Z"},"trusted":true},"outputs":[],"source":["fig = show_data(5,4,is_train=False)\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["# PyTorch Datasets and Dataloaders\n","The dataset which is provided to us is just images in separate folders: `freshapples`,`freshbanana`,`freshoranges`,`rottenapples`,`rottenbanana` & `rottenoranges`. These folders are common subfolders in train and test folders.\n","\n","A lot of effort in solving any machine learning problem goes in to preparing the data. PyTorch provides many tools to make data loading easy and hopefully, to make your code more readable.\n","\n","## Custom Dataset\n","`torch.utils.data.Dataset` is an abstract class representing a dataset. The custom dataset should inherit Dataset and override the following methods:\n","\n","- `__len__` so that len(dataset) returns the size of the dataset.\n","- `__getitem__` to support the indexing such that dataset[i] can be used to get ith sample\n","\n","So, I'll be creating a custom dataset `FruitsDataset` which inherits Dataset class and overrides the above methods."]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2022-07-23T04:14:46.859859Z","iopub.status.busy":"2022-07-23T04:14:46.859509Z","iopub.status.idle":"2022-07-23T04:14:46.877742Z","shell.execute_reply":"2022-07-23T04:14:46.876761Z","shell.execute_reply.started":"2022-07-23T04:14:46.859825Z"},"trusted":true},"outputs":[],"source":["class FruitsDataset(Dataset):\n","    def __init__(self,path,classifier_type='Rotten',subset='train',transforms=None):\n","        self.subset = subset\n","        if self.subset == 'train':\n","            self.PATH = os.path.join(path,'train','*','*.png')\n","        elif self.subset == 'test':\n","            self.PATH = os.path.join(path,'test','*','*.png')\n","        self.data = glob.glob(self.PATH)\n","        self.height = 32\n","        self.width = 32\n","        self.labels = [] \n","        if classifier_type == 'Rotten':\n","            classes = ['fresh','rotten']\n","            for fruit in self.data:\n","                if classes[0] in fruit.split('/')[-2]:\n","                    self.labels.append(0)\n","                else:\n","                    self.labels.append(1)\n","        else:\n","            classes = ['apple','banana','orange']\n","            for fruit in self.data:\n","                if classes[0] in fruit:\n","                    self.labels.append(0)\n","                elif classes[1] in fruit:\n","                    self.labels.append(1)\n","                else:\n","                    self.labels.append(2)\n","        self.transforms = transforms\n","      \n","    def __getitem__(self,index):\n","        img_path = self.data[index]\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img,(self.width,self.height))\n","        label = self.labels[index]\n","        if self.transforms is not None:\n","            img_as_tensor = self.transforms(img)\n","            if self.transforms is not None:\n","                return(img_as_tensor,label)\n","            return(img,label)\n","  \n","    def __len__(self):\n","        return(len(self.data))"]},{"cell_type":"markdown","metadata":{},"source":["## Transforms\n","One issue we can face is the samples are not of the same size. Most neural networks expect the images of a fixed size. Therefore, we will need to write some prepocessing code. Letâ€™s create two transforms:\n","\n","- `ToTensor`: to convert the numpy images to torch images (we need to swap axes).\n","- `Normalize`: so that the images have zero mean and one variance.\n","\n","**Please Note: ** For rescaling the images to 32x32 size(This is the size I have chosen) I have used cv2. Instead, we can simply use `Rescale()` transform "]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-07-23T04:14:46.879772Z","iopub.status.busy":"2022-07-23T04:14:46.879470Z","iopub.status.idle":"2022-07-23T04:14:46.954922Z","shell.execute_reply":"2022-07-23T04:14:46.953973Z","shell.execute_reply.started":"2022-07-23T04:14:46.879722Z"},"trusted":true},"outputs":[],"source":["transformations = transforms.Compose([\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize((0.7321, 0.6322, 0.5291),\n","                                                           (0.3302, 0.3432, 0.3701))\n","                                      ])\n","dataset = FruitsDataset('/kaggle/input/fruits-fresh-and-rotten-for-classification/dataset/',transforms = transformations)"]},{"cell_type":"markdown","metadata":{},"source":["Let's take a look at a random image from the dataset we have created. I have used permute function because images are usually represented as `Height x Width x #Channels` where #Channels is 3 for RGB images and 1 for grayscale images. While, pytorch tensors are represented as `#Channels x Height x Width`."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-07-23T04:14:46.956561Z","iopub.status.busy":"2022-07-23T04:14:46.956255Z","iopub.status.idle":"2022-07-23T04:14:47.160082Z","shell.execute_reply":"2022-07-23T04:14:47.159055Z","shell.execute_reply.started":"2022-07-23T04:14:46.956530Z"},"trusted":true},"outputs":[],"source":["img_t, _ = dataset[1000]\n","img = img_t.permute(1,2,0)\n","plt.imshow(img);"]},{"cell_type":"markdown","metadata":{},"source":["You will be seeing a very distored image of a fruit. Its because of the transformations applied to have a common size for all the images. I have chosen such a small size for reducing the training time."]},{"cell_type":"markdown","metadata":{},"source":["## DataLoaders\n","We can iterate over the created dataset with a simple `for` loop. However, we are losing a lot of features by using a simple `for` loop to iterate over the data. In particular, we are missing out on:\n","\n","- Batching the data\n","- Shuffling the data\n","- Load the data in parallel using multiprocessing workers.\n","\n","`torch.utils.data.DataLoader` is an iterator which provides all these features. I will create separate loaders for training and validation with sampling."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-07-23T04:14:47.162066Z","iopub.status.busy":"2022-07-23T04:14:47.161623Z","iopub.status.idle":"2022-07-23T04:14:47.172737Z","shell.execute_reply":"2022-07-23T04:14:47.171854Z","shell.execute_reply.started":"2022-07-23T04:14:47.162021Z"},"trusted":true},"outputs":[],"source":["batch_size = 64\n","validation_split = .2\n","shuffle_dataset = True\n","random_seed= 42\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n","                                           sampler=train_sampler)\n","validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                                sampler=valid_sampler)"]},{"cell_type":"markdown","metadata":{},"source":["# Training the Model\n","Let's redifine the `nn.Module` class to create our custom NN."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-07-23T04:14:47.174210Z","iopub.status.busy":"2022-07-23T04:14:47.173925Z","iopub.status.idle":"2022-07-23T04:14:47.185774Z","shell.execute_reply":"2022-07-23T04:14:47.184848Z","shell.execute_reply.started":"2022-07-23T04:14:47.174181Z"},"trusted":true},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3,16,kernel_size=3,padding=1)\n","        self.conv2 = nn.Conv2d(16,8,kernel_size=3,padding=1)\n","        self.fc1 = nn.Linear(8*8*8,32)\n","        self.fc2 = nn.Linear(32,2)\n","    def forward(self,x):\n","        out = F.max_pool2d(torch.tanh(self.conv1(x)),2)\n","        out = F.max_pool2d(torch.tanh(self.conv2(out)),2)\n","        out = out.view(-1,8*8*8)\n","        out = torch.tanh(self.fc1(out))\n","        out = self.fc2(out)\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["Lets check the number of parameters we have to train."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-07-23T04:14:47.187737Z","iopub.status.busy":"2022-07-23T04:14:47.187348Z","iopub.status.idle":"2022-07-23T04:14:47.204211Z","shell.execute_reply":"2022-07-23T04:14:47.203440Z","shell.execute_reply.started":"2022-07-23T04:14:47.187693Z"},"trusted":true},"outputs":[],"source":["model = Net()\n","numel_list = [p.numel() for p in model.parameters()]\n","sum(numel_list), numel_list"]},{"cell_type":"markdown","metadata":{},"source":["Confirm if we are training on GPU."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-07-23T04:14:47.205460Z","iopub.status.busy":"2022-07-23T04:14:47.205177Z","iopub.status.idle":"2022-07-23T04:14:47.215135Z","shell.execute_reply":"2022-07-23T04:14:47.213871Z","shell.execute_reply.started":"2022-07-23T04:14:47.205432Z"},"trusted":true},"outputs":[],"source":["device = (torch.device('cuda') if torch.cuda.is_available()\n","  else torch.device('cpu'))\n","print(f\"Training on device {device}.\")"]},{"cell_type":"markdown","metadata":{},"source":["Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-23T04:14:47.217037Z","iopub.status.busy":"2022-07-23T04:14:47.216697Z"},"trusted":true},"outputs":[],"source":["def training_loop(n_epochs,optimizer,model,loss_fn,train_loader):\n","    for epoch in tqdm_notebook(range(1,n_epochs+1)):\n","        loss_train = 0.0\n","        for imgs, labels in train_loader:\n","            imgs = imgs.to(device=device)\n","            labels = labels.to(device=device)\n","            outputs = model(imgs)\n","            loss = loss_fn(outputs,labels)\n","            #get rid of gradients from last round\n","            optimizer.zero_grad()\n","            #performs backward step. Computes all the gradients\n","            loss.backward()\n","            #Updates the model\n","            optimizer.step()\n","            loss_train += loss.item()\n","        print('{} Epoch {}, Training Loop {}'.format(\n","          datetime.datetime.now(), epoch, loss_train/len(train_loader)))\n","\n","model = Net().to(device=device) #was talking about this above\n","optimizer = optim.SGD(model.parameters(), lr=1e-2)\n","loss_fn = nn.CrossEntropyLoss()\n","training_loop(\n","  n_epochs = 50,\n","  optimizer = optimizer,\n","  model = model,\n","  loss_fn = loss_fn,\n","  train_loader = train_loader,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Save our trained model to be deployed on the site later on."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), 'FreshnessDetector.pt')"]},{"cell_type":"markdown","metadata":{},"source":["Validating our model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def validate(model,train_loader,val_loader):\n","    for name, loader in [('train',train_loader),('val',validation_loader)]:\n","        correct = 0\n","        total = 0\n","\n","        #gradients nor required, as we don't want to train our parameters\n","        with torch.no_grad():\n","            for imgs, labels in loader:\n","                imgs = imgs.to(device=device)\n","                labels = labels.to(device=device)\n","                outputs = model(imgs)\n","                #max_index,value\n","                _,predicted = torch.max(outputs,dim=1)\n","                total+=labels.shape[0]\n","                correct+=int((predicted==labels).sum())\n","    \n","        print('Accuracy {}: {:.2f}'.format(name, correct/total))\n","\n","validate(model,train_loader,validation_loader)"]},{"cell_type":"markdown","metadata":{},"source":["Let's check one sample."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img,label = dataset[1]\n","plt.imshow(img.permute(1,2,0))\n","out = model(img.unsqueeze(0).to(device))\n","print('Actual: {}'.format(label))\n","print('Prediction: {}'.format(out))"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluating our model\n","Apply the same transformation on the test dataset. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["transformations_test = transforms.Compose([\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize((0.7369, 0.6360, 0.5318),\n","                                                           (0.3281, 0.3417, 0.3704))\n","                                      ])\n","test = FruitsDataset('/kaggle/input/fruits-fresh-and-rotten-for-classification/dataset/',subset='test',transforms=transformations_test)"]},{"cell_type":"markdown","metadata":{},"source":["The values sent to normalize are calculated using the code below."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# imgs = torch.stack([img for img,_ in tqdm_notebook(test)], dim=3)\n","# imgs.shape\n","\n","# #Mean\n","# print(imgs.view(3,-1).mean(dim=1))\n","\n","# #Standard Deviation\n","# print(imgs.view(3, -1).std(dim=1))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img,_ = test[400]\n","plt.imshow(img.permute(1,2,0))\n","s = nn.Softmax(dim=1)\n","out = s(model(img.unsqueeze(0).to(device)))\n","print('Prediction: {}'.format(out))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["correct = 0\n","total = 0\n","test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size)\n","for imgs, labels in train_loader:\n","    imgs = imgs.to(device=device)\n","    labels = labels.to(device=device)\n","    out = model(imgs)\n","    _,predicted = torch.max(out,dim=1)\n","    correct += int((predicted==labels).sum())\n","    total += len(labels)"]},{"cell_type":"markdown","metadata":{},"source":["Our final accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["(correct/total)*100"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
